{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/Users/trippgordon/Desktop/Code/sonic/NoisyGAN\n",
      "\u001b[1m\u001b[36mData\u001b[m\u001b[m        README.md   \u001b[1m\u001b[36mcore\u001b[m\u001b[m        \u001b[1m\u001b[36mmodels\u001b[m\u001b[m      submit.sh\r\n",
      "\u001b[1m\u001b[36mNotebooks\u001b[m\u001b[m   __init__.py \u001b[1m\u001b[36mdepricated\u001b[m\u001b[m  \u001b[1m\u001b[36mnotes_ideas\u001b[m\u001b[m train.py\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:tf versionn: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "%cd ../..\n",
    "!ls\n",
    "from core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binary_crossentropy(tf.keras.layers.Layer):\n",
    "    def __init__(self,name = \"binary_crossentropy\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def call(self, target, output):\n",
    "        return tf.keras.losses.binary_crossentropy(target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Discriminator(ddsp.training.models.Model):\n",
    "    \"\"\"for now, just re-encode the fake sample. \n",
    "        in the future, just feed flz from fake right in\n",
    "        TODO: subclass ddsp Model\n",
    "    \"\"\"\n",
    "    def __init__(self, losses=[binary_crossentropy()]):\n",
    "        super().__init__(name='discriminator', losses=losses)\n",
    "        self.preprocessor = self.buildPreprocessor()\n",
    "        self.flzEncoder = self.buildFLZEncoder()\n",
    "        self.classifier = self.buildClassifier()\n",
    "        \n",
    "    def call(self, sample, training=True): \n",
    "        print(tf.executing_eagerly())\n",
    "        # audio key must have shape (1,64000)\n",
    "        preprocessed = self.preprocessor(sample)\n",
    "        describeSample(preprocessed)\n",
    "        \n",
    "        # only get flz if label=1? ... try both\n",
    "        encoded = self.flzEncoder(preprocessed)\n",
    "        \n",
    "        # shape the shit into [1, 1000, 8, 1]\n",
    "        encoded_concat = tf.concat([encoded['f0_scaled'],\n",
    "                                    encoded['ld_scaled'],\n",
    "                                    encoded['z']], axis=2)\n",
    "        encoded_concat = tf.expand_dims(encoded_concat,axis=3)\n",
    "        \n",
    "        # classify if it's real or not\n",
    "        classification = self.classifier(encoded_concat, training=training)\n",
    "        \n",
    "        if training:\n",
    "            label = sample['label']\n",
    "            print(label,classification)\n",
    "            self.add_losses(label, classification)\n",
    "            \n",
    "        return classification\n",
    "        \n",
    "    def buildPreprocessor(self):\n",
    "        return ddsp.training.preprocessing.DefaultPreprocessor(time_steps=1000)\n",
    "\n",
    "    def buildFLZEncoder(self):\n",
    "        # TODO: try giving this an f0 encoder, like in ae_abs.gin\n",
    "        encoder = ddsp.training.encoders.MfccTimeDistributedRnnEncoder(z_dims=6,\n",
    "                                                                       z_time_steps=1000)\n",
    "        return encoder\n",
    "    \n",
    "    def buildClassifier(self,training=True):\n",
    "        #TODO \n",
    "        # now encode even further down to a binary classification real or fake\n",
    "        discriminator = Sequential()\n",
    "        discriminator.add(InputLayer(((1000,8,1)), batch_size=1))\n",
    "        # downsample to 500x3\n",
    "        discriminator.add(Conv2D(16, (3,3), strides=(2, 2), padding='same'))\n",
    "        discriminator.add(BatchNormalization())\n",
    "        discriminator.add(LeakyReLU(alpha=0.2))\n",
    "        # downsample to 250 x 2\n",
    "        discriminator.add(Conv2D(16, (3,3), strides=(2, 2), padding='same'))\n",
    "        discriminator.add(BatchNormalization())\n",
    "        discriminator.add(LeakyReLU(alpha=0.2))\n",
    "        # downsample to 125 x 1\n",
    "        discriminator.add(Conv2D(16, (3,3), strides=(2, 2), padding='same'))\n",
    "        discriminator.add(BatchNormalization())\n",
    "        discriminator.add(LeakyReLU(alpha=0.2))\n",
    "        # classify\n",
    "        discriminator.add(Flatten())\n",
    "        discriminator.add(Dense(1, activation='sigmoid'))\n",
    "#         discriminator.summary()\n",
    "        \n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio: (64000,)\n",
      "f0_hz: (1000,)\n",
      "loudness_db: (1000,)\n",
      "label: (1,)\n"
     ]
    }
   ],
   "source": [
    "output_tfrecord_path = './Data/piano/piano30s.tfrecord'\n",
    "dataset = DDSP_DATASET(output_tfrecord_path)\n",
    "\n",
    "dataset_iter = iter(dataset.data_provider.get_batch(1, shuffle=True, repeats=-1))\n",
    "# dataset = trainer.distribute_dataset(dataset)\n",
    "\n",
    "real_sample = dataset.getSample(sampleNum=0)\n",
    "describeSample(real_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bb0846ed671c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDSP_TRAINER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./models/disc/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "trainer = DDSP_TRAINER(model_dir=\"./models/disc/\",model=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "audio: (1, 64000)\n",
      "f0_confidence: (1, 1000)\n",
      "f0_hz: (1, 1000, 1)\n",
      "loudness_db: (1, 1000, 1)\n",
      "label: (1, 1)\n",
      "f0_scaled: (1, 1000, 1)\n",
      "ld_scaled: (1, 1000, 1)\n",
      "Tensor(\"fn_3:0\", shape=(1, 1), dtype=int32) Tensor(\"discriminator/sequential/dense/Sigmoid:0\", shape=(1, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "INFO:tensorflow:Error reported to Coordinator: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n",
      "array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n",
      "          -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n",
      "           0.09575431,  0.04282896, -0.05541672,  0.09882917,\n",
      "           0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n",
      "\n",
      "        [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n",
      "           0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n",
      "           0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n",
      "          -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n",
      "\n",
      "        [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n",
      "           0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n",
      "          -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n",
      "           0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n",
      "\n",
      "\n",
      "       [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n",
      "           0.08987354, -0.16798228, -0.01053625,  0.00421502,\n",
      "           0.19706087,  0.18109547,  0.05307396,  0.11995952,\n",
      "          -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n",
      "\n",
      "        [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n",
      "           0.11989756, -0.17968275,  0.02814747,  0.01529613,\n",
      "          -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n",
      "          -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n",
      "\n",
      "        [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n",
      "          -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n",
      "           0.16340037,  0.05284129,  0.15783785, -0.17008543,\n",
      "           0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n",
      "\n",
      "\n",
      "       [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n",
      "          -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n",
      "           0.13073169, -0.04129952,  0.13888638,  0.10465564,\n",
      "           0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n",
      "\n",
      "        [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n",
      "          -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n",
      "          -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n",
      "           0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n",
      "\n",
      "        [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n",
      "          -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n",
      "          -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n",
      "          -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n",
      "      dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3206, in bound_method_wrapper\n",
      "    return wrapped_fn(weak_instance(), *args, **kwargs)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py\", line 227, in step_fn\n",
      "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 434, in apply_gradients\n",
      "    self._create_slots(var_list)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py\", line 149, in _create_slots\n",
      "    self.add_slot(var, 'm')\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 592, in add_slot\n",
      "    .format(strategy, var))\n",
      "ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n",
      "array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n",
      "          -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n",
      "           0.09575431,  0.04282896, -0.05541672,  0.09882917,\n",
      "           0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n",
      "\n",
      "        [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n",
      "           0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n",
      "           0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n",
      "          -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n",
      "\n",
      "        [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n",
      "           0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n",
      "          -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n",
      "           0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n",
      "\n",
      "\n",
      "       [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n",
      "           0.08987354, -0.16798228, -0.01053625,  0.00421502,\n",
      "           0.19706087,  0.18109547,  0.05307396,  0.11995952,\n",
      "          -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n",
      "\n",
      "        [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n",
      "           0.11989756, -0.17968275,  0.02814747,  0.01529613,\n",
      "          -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n",
      "          -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n",
      "\n",
      "        [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n",
      "          -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n",
      "           0.16340037,  0.05284129,  0.15783785, -0.17008543,\n",
      "           0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n",
      "\n",
      "\n",
      "       [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n",
      "          -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n",
      "           0.13073169, -0.04129952,  0.13888638,  0.10465564,\n",
      "           0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n",
      "\n",
      "        [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n",
      "          -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n",
      "          -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n",
      "           0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n",
      "\n",
      "        [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n",
      "          -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n",
      "          -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n",
      "          -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n",
      "      dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n",
      "array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n",
      "          -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n",
      "           0.09575431,  0.04282896, -0.05541672,  0.09882917,\n",
      "           0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n",
      "\n",
      "        [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n",
      "           0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n",
      "           0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n",
      "          -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n",
      "\n",
      "        [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n",
      "           0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n",
      "          -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n",
      "           0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n",
      "\n",
      "\n",
      "       [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n",
      "           0.08987354, -0.16798228, -0.01053625,  0.00421502,\n",
      "           0.19706087,  0.18109547,  0.05307396,  0.11995952,\n",
      "          -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n",
      "\n",
      "        [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n",
      "           0.11989756, -0.17968275,  0.02814747,  0.01529613,\n",
      "          -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n",
      "          -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n",
      "\n",
      "        [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n",
      "          -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n",
      "           0.16340037,  0.05284129,  0.15783785, -0.17008543,\n",
      "           0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n",
      "\n",
      "\n",
      "       [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n",
      "          -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n",
      "           0.13073169, -0.04129952,  0.13888638,  0.10465564,\n",
      "           0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n",
      "\n",
      "        [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n",
      "          -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n",
      "          -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n",
      "           0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n",
      "\n",
      "        [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n",
      "          -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n",
      "          -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n",
      "          -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n",
      "      dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3206, in bound_method_wrapper\n",
      "    return wrapped_fn(weak_instance(), *args, **kwargs)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py\", line 227, in step_fn\n",
      "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 434, in apply_gradients\n",
      "    self._create_slots(var_list)\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py\", line 149, in _create_slots\n",
      "    self.add_slot(var, 'm')\n",
      "  File \"/Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 592, in add_slot\n",
      "    .format(strategy, var))\n",
      "ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n",
      "array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n",
      "          -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n",
      "           0.09575431,  0.04282896, -0.05541672,  0.09882917,\n",
      "           0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n",
      "\n",
      "        [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n",
      "           0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n",
      "           0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n",
      "          -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n",
      "\n",
      "        [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n",
      "           0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n",
      "          -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n",
      "           0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n",
      "\n",
      "\n",
      "       [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n",
      "           0.08987354, -0.16798228, -0.01053625,  0.00421502,\n",
      "           0.19706087,  0.18109547,  0.05307396,  0.11995952,\n",
      "          -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n",
      "\n",
      "        [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n",
      "           0.11989756, -0.17968275,  0.02814747,  0.01529613,\n",
      "          -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n",
      "          -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n",
      "\n",
      "        [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n",
      "          -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n",
      "           0.16340037,  0.05284129,  0.15783785, -0.17008543,\n",
      "           0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n",
      "\n",
      "\n",
      "       [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n",
      "          -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n",
      "           0.13073169, -0.04129952,  0.13888638,  0.10465564,\n",
      "           0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n",
      "\n",
      "        [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n",
      "          -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n",
      "          -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n",
      "           0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n",
      "\n",
      "        [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n",
      "          -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n",
      "          -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n",
      "          -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n",
      "      dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:213 train_step  *\n        losses = self.run(self.step_fn, batch)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:194 run  *\n        return self.strategy.experimental_run_v2(fn, args=args, kwargs=kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:763 experimental_run_v2\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:693 _call_for_each_replica  *\n        return _call_for_each_replica(self._container_strategy(), self._device_map,\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:201 _call_for_each_replica\n        coord.join(threads)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/six.py:693 reraise\n        raise value\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:917 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:3206 bound_method_wrapper\n        return wrapped_fn(weak_instance(), *args, **kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:227 step_fn\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients\n        self._create_slots(var_list)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py:149 _create_slots\n        self.add_slot(var, 'm')\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:592 add_slot\n        .format(strategy, var))\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n    array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n              -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n               0.09575431,  0.04282896, -0.05541672,  0.09882917,\n               0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n    \n            [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n               0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n               0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n              -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n    \n            [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n               0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n              -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n               0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n    \n    \n           [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n               0.08987354, -0.16798228, -0.01053625,  0.00421502,\n               0.19706087,  0.18109547,  0.05307396,  0.11995952,\n              -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n    \n            [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n               0.11989756, -0.17968275,  0.02814747,  0.01529613,\n              -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n              -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n    \n            [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n              -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n               0.16340037,  0.05284129,  0.15783785, -0.17008543,\n               0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n    \n    \n           [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n              -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n               0.13073169, -0.04129952,  0.13888638,  0.10465564,\n               0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n    \n            [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n              -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n              -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n               0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n    \n            [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n              -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n              -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n              -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-613c0d6eaf91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 497\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:213 train_step  *\n        losses = self.run(self.step_fn, batch)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:194 run  *\n        return self.strategy.experimental_run_v2(fn, args=args, kwargs=kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:763 experimental_run_v2\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:693 _call_for_each_replica  *\n        return _call_for_each_replica(self._container_strategy(), self._device_map,\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:201 _call_for_each_replica\n        coord.join(threads)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/six.py:693 reraise\n        raise value\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:917 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:3206 bound_method_wrapper\n        return wrapped_fn(weak_instance(), *args, **kwargs)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/ddsp/training/train_util.py:227 step_fn\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients\n        self._create_slots(var_list)\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py:149 _create_slots\n        self.add_slot(var, 'm')\n    /Users/trippgordon/miniconda3/envs/ddsp/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:592 add_slot\n        .format(strategy, var))\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fb8d1636438>), which is different from the scope used for the original variable (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 16) dtype=float32, numpy=\n    array([[[[ 0.06731801, -0.04024038, -0.01980495,  0.1398225 ,\n              -0.08543971, -0.08174239, -0.02340412,  0.07293941,\n               0.09575431,  0.04282896, -0.05541672,  0.09882917,\n               0.01430061, -0.01578866, -0.17067254, -0.00148062]],\n    \n            [[-0.19512598, -0.13727483,  0.01120239,  0.11547579,\n               0.14468493,  0.0314465 , -0.09951366,  0.0858347 ,\n               0.0996217 ,  0.14021583, -0.14919347, -0.16672489,\n              -0.02499968, -0.0345273 , -0.19485097, -0.09858931]],\n    \n            [[ 0.08179651,  0.18268053, -0.10176128, -0.1793624 ,\n               0.0109522 ,  0.01742311,  0.08951022, -0.08096275,\n              -0.08729522,  0.05820493, -0.13500318,  0.04449444,\n               0.15894346,  0.05901305,  0.12642379,  0.12459661]]],\n    \n    \n           [[[ 0.0444984 , -0.02884991,  0.07311703, -0.08248625,\n               0.08987354, -0.16798228, -0.01053625,  0.00421502,\n               0.19706087,  0.18109547,  0.05307396,  0.11995952,\n              -0.00483716, -0.104569  ,  0.13063295,  0.07938401]],\n    \n            [[ 0.0639955 ,  0.1441604 , -0.06739753,  0.09173809,\n               0.11989756, -0.17968275,  0.02814747,  0.01529613,\n              -0.15477887, -0.04226485,  0.15871646,  0.16467698,\n              -0.01905875,  0.11170168, -0.12337576,  0.12127794]],\n    \n            [[-0.16419932,  0.1524914 , -0.08804979,  0.11354502,\n              -0.14200675,  0.06532739,  0.14778511, -0.08374738,\n               0.16340037,  0.05284129,  0.15783785, -0.17008543,\n               0.06956215, -0.15879054,  0.12335585,  0.01767136]]],\n    \n    \n           [[[ 0.14283578, -0.10822774, -0.02531458,  0.05191235,\n              -0.10662507,  0.13129987, -0.03119385,  0.09359066,\n               0.13073169, -0.04129952,  0.13888638,  0.10465564,\n               0.00541732,  0.11402954,  0.06704573, -0.17874148]],\n    \n            [[-0.00333641,  0.16679339, -0.0243196 , -0.00679569,\n              -0.00801116,  0.09399204,  0.12963425,  0.04971516,\n              -0.10434544, -0.179608  ,  0.09410615, -0.09870928,\n               0.1171077 , -0.15134831, -0.13301688,  0.10556705]],\n    \n            [[-0.17320816, -0.06439595,  0.00889198,  0.06102361,\n              -0.04463707, -0.11362628, -0.15811151,  0.18285112,\n              -0.18527623, -0.19661361,  0.18250926,  0.09861664,\n              -0.15185684, -0.01086864,  0.17475493,  0.02281673]]]],\n          dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\n"
     ]
    }
   ],
   "source": [
    "trainer.train_step(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:tf versionn: 2.1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d840f20d300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "trainer.train(dataset, batch_size=1, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_step\n",
    "grad_clip_norm = 3.0\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "with tf.GradientTape() as tape:\n",
    "    _ = discriminator(real_sample,training=True)\n",
    "    total_loss = tf.reduce_sum(discriminator.losses)\n",
    "    print(total_loss)\n",
    "grads = tape.gradient(total_loss, discriminator.trainable_variables)\n",
    "grads, _ = tf.clip_by_global_norm(grads, grad_clip_norm)\n",
    "opt.apply_gradients(zip(grads, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, prediction = trainer.predict(dataset)\n",
    "play(audio)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
